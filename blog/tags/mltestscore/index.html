<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="generator" content="Docusaurus v2.0.0-alpha.58">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Queuery Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Queuery Blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156581645-3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-156581645-3",{})</script><title data-react-helmet="true">Posts tagged &quot;mltestscore&quot; | Queuery</title><meta data-react-helmet="true" property="og:title" content="Posts tagged &quot;mltestscore&quot; | Queuery"><meta data-react-helmet="true" name="description" content="Blog | Tagged &quot;mltestscore&quot;"><meta data-react-helmet="true" property="og:description" content="Blog | Tagged &quot;mltestscore&quot;"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/styles.e5e64cf7.css">
<link rel="preload" href="/styles.4398f2ec.js" as="script">
<link rel="preload" href="/runtime~main.8f6c4829.js" as="script">
<link rel="preload" href="/main.84f1f6c6.js" as="script">
<link rel="preload" href="/1.c2046979.js" as="script">
<link rel="preload" href="/2.89f7a116.js" as="script">
<link rel="preload" href="/3.0cdb0c07.js" as="script">
<link rel="preload" href="/6875c492.2f7c3f5d.js" as="script">
<link rel="preload" href="/7f5d2955.8c68583d.js" as="script">
<link rel="preload" href="/f610b5ae.676be50c.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=window.matchMedia("(prefers-color-scheme: dark)"),n=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==n?t(n):e.matches&&t("dark")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/logo.png" alt="My Site Logo"><strong class="navbar__title">Queuery</strong></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/na0fu3y/queuery" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_1gtM"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_keGJ moon_1gwN"></span></div><div class="react-toggle-track-x"><span class="toggle_keGJ sun_3CPA"></span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/logo.png" alt="My Site Logo"><strong class="navbar__title">Queuery</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/introduction">Docs</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/na0fu3y/queuery" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><main class="col col--8 col--offset-2"><h1>1 post tagged with &quot;mltestscore&quot;</h1><a href="/blog/tags">View All Tags</a><div class="margin-vert--xl"><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_2RZH"><a href="/blog/ml-test-score">機械学習システムの技術的負債を評価する方法論</a></h2><div class="margin-vert--md"><time datetime="2020-07-01T00:00:00.000Z" class="blogPostDate_3tRe">July 1, 2020  · 2 min read</time></div><div class="avatar margin-vert--md"><a class="avatar__photo-link avatar__photo" href="https://github.com/na0fu3y" target="_blank" rel="noreferrer noopener"><img src="https://avatars0.githubusercontent.com/u/17900178?s=400&amp;v=4" alt="Naofumi Yamada"></a><div class="avatar__intro"><h4 class="avatar__name"><a href="https://github.com/na0fu3y" target="_blank" rel="noreferrer noopener">Naofumi Yamada</a></h4><small class="avatar__subtitle">Data Engineer</small></div></div></header><section class="markdown"><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="はじめに"></a>はじめに<a aria-hidden="true" tabindex="-1" class="hash-link" href="#はじめに" title="Direct link to heading">#</a></h1><p>この記事は、使われている機械学習システムが技術的負債になっていないか、レビューできる観点をまとめたものです。</p><p>技術的負債は定量化が難しく、また負債を返済するための優先順位付けや改善程度の測定も難しいです。そのガイドラインとして利用可能なML Test Scoreの知名度を高め、活用できたらと思っています。</p><p>もちろんですが、私自身は評価自体が、スケールビジネスの発見より優先度の高いものだとは思いません。ビジネスのフェーズに合わせて、適切なレベルにあっているかの評価に使えることを願います。</p><p>ベース記事は<a href="https://tech.mercari.com/entry/2020/06/17/150000" target="_blank" rel="noopener noreferrer">機械学習システムの信頼性を数値化するML Test Scoreのハンズオンワークショップを開催しました@Mercari Engineering Blog</a>です。</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="機械学習システムの技術的負債を評価する"></a>機械学習システムの技術的負債を評価する<a aria-hidden="true" tabindex="-1" class="hash-link" href="#機械学習システムの技術的負債を評価する" title="Direct link to heading">#</a></h1><p>機械学習システムの技術的負債を可視化し現状を把握するためのフレームワークに、ML Test Scoreがあります。<a href="https://research.google/pubs/pub46555/" target="_blank" rel="noopener noreferrer">The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction@Google Research</a></p><p>これは「教師ありの機械学習モデルで推論を行い、リアルタイムに予測結果を返すシステム」について評価するフレームワークですので、会社によってはやや乖離があるかもしれません。
乖離がある場合には全社的に議論を進め、ドメインに合わせた評価になるような仕組みになったら嬉しいです。</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="ml-test-score"></a>ML Test Score<a aria-hidden="true" tabindex="-1" class="hash-link" href="#ml-test-score" title="Direct link to heading">#</a></h1><p>4つのパートから構成され、各パート7つの検査項目が提案されています。</p><ul><li>特徴量とデータのテスト</li><li>モデル開発のテスト</li><li>機械学習システムインフラのテスト</li><li>機械学習のモニタリングテスト</li></ul><p>検査項目は、「手動で確認され、ドキュメントに結果をまとめ配布されている場合」は、0.5ポイント、「反復的に自動的に検証されている場合」は1.0ポイントになります。
4つのパートそれぞれに合計を計算し、4パートのスコアの最小値が最終的な ML Test Score です。全てのパートが重要で、全ての検査項目が同一の価値を持つためパートごとの合計の最小値を選択します。</p><p>以下が、ML Test Scoreにおける、機械学習システムがどのレベルに達しているかの説明です。</p><table><thead><tr><th>ポイント</th><th>説明</th></tr></thead><tbody><tr><td>0</td><td>プロダクションレベルというよりも、研究プロジェクトの一種</td></tr><tr><td>(0,1]</td><td>総合的にテストはされていないが、可能な限り信頼性向上に努めている</td></tr><tr><td>(1,2]</td><td>基礎的なプロジェクトの要求事項は通過した。しかし、信頼性向上のためのさらなる投資が必要とされる</td></tr><tr><td>(2,3]</td><td>適切なテストがされている、だが更に自動化の余地が残っている</td></tr><tr><td>(3,5]</td><td>信頼性の高い自動化されたテストとモニタリングレベル。ミッションクリティカルな状況でも問題はない</td></tr><tr><td>&gt;5</td><td>卓越したレベルの機械学習システム</td></tr></tbody></table><p>それでは具体的な項目をみていきましょう。</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="特徴量とデータのテスト"></a>特徴量とデータのテスト<a aria-hidden="true" tabindex="-1" class="hash-link" href="#特徴量とデータのテスト" title="Direct link to heading">#</a></h2><table><thead><tr><th>Data</th><th>How</th></tr></thead><tbody><tr><td>Data 1: 期待する特徴量は全てスキーマで管理され、読み込み可能か?</td><td>このスキーマは学習データの統計値を計算することに有用である。</td></tr><tr><td>Data 2: 全ての特徴量は有用か?全ての特徴量は、追加すればするほどコストになる。独立した全ての特徴量は有用だろうか?</td><td>特徴量削除や目的変数と特徴量の相関などを見てみよう</td></tr><tr><td>Data 3: 特徴量のコストは高くないか?</td><td>推論速度や RAM の使用率だけを見るのではなく、その特徴量に関連するデータの依存先や不安定性なども考慮する</td></tr><tr><td>Data 4: 特徴量は要件に準拠しているか?一般的には、機械学習システムにデータが送られる際には単一のリソースからデータを取得することが前提ですが、実験の際には知らずしらずのうちに精度を向上させるために特徴量をどんどん追加してしまいがちである</td><td>学習データのリソースがプロダクション環境から逸脱していないか監視する</td></tr><tr><td>Data 5: データパイプラインは適切なプライバシーコントロールはされているか?</td><td>データパイプライン構築の時には権限管理を厳密に行う</td></tr><tr><td>Data 6: 新しい特徴量は素早く追加可能か?</td><td>新しいアイデアを素早く試せるチームは強い。世界規模、またはトラフィックが多い機械学習システムでも、1-2 ヶ月で新しい機能を追加することが可能である。Data 5 と相反する</td></tr><tr><td>Data 7: 全ての特徴量生成コードはテストされているか?</td><td>特徴量生成のコードは一見シンプルでユニットテストなどは必要そうに見えないかもしれないが、ここで発生するバグは発見することはとても難しい</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="モデル開発のテスト"></a>モデル開発のテスト<a aria-hidden="true" tabindex="-1" class="hash-link" href="#モデル開発のテスト" title="Direct link to heading">#</a></h2><table><thead><tr><th>Model</th><th>How</th></tr></thead><tbody><tr><td>Model 1: 全てのモデルはレビューされ、リポジトリに格納されている。バージョン管理され、過去の実験との比較や再現実験などを可能にする</td><td></td></tr><tr><td>Model 2: オフラインの疑似指標はオンラインメトリクスに相関しているか?</td><td>A/B テストで意図的にモデルのスコアを劣化させてテストする</td></tr><tr><td>Model 3: 全てのハイパーパラメータはチューニングされているか?学習率・NN の層の数・次元数など数多くのハイパーパラメータがあり、予測精度に大きな影響を与える</td><td>グリッドサーチや確率的なパラメータチューニングをおこなう</td></tr><tr><td>Model 4: 古くなった(陳腐)モデルの影響は把握されているか?例えば学習パイプラインが失敗し、更新されなかったモデルをここで古びたモデルと呼ぶ。モデルが更新されないことでどれくらい予測精度に影響を与えるか理解することで、再学習の適切な期間も決定できる。ほとんどのモデルは外部世界の要因により更新しなければならない。</td><td>小規模な A/B テストにより、新モデルと旧モデルを比較することでモデルのリフレッシュネスがどれくらい重要か計測することができる</td></tr><tr><td>Model 5: 単純なモデルは必ずしも良くはない。少ない特徴量で単純な線形モデルがベースラインとして使われることがあるが、パイプライン構築のコストとその利益のトレードオフを常に考えていくべきである</td><td></td></tr><tr><td>Model 6: 全ての重要なデータの部分集合でもモデルの質は十分なものになっているか?</td><td>多様な基準でデータを抜き出し、モデルの精度が変化しないかを確認する。例えば、時間帯や性別、年齢などの基準でデータを抜き出してもモデルの精度は同一か？</td></tr><tr><td>Model 7: 包括性を考慮したモデルになっているか?機械学習システムでは Fairness などが最近課題となっている。</td><td>特定のグループ間で、実験をおこない結果が異なっていないか確認する。</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="機械学習システムインフラのテスト"></a>機械学習システムインフラのテスト<a aria-hidden="true" tabindex="-1" class="hash-link" href="#機械学習システムインフラのテスト" title="Direct link to heading">#</a></h2><table><thead><tr><th>Infra</th><th>How</th></tr></thead><tbody><tr><td>Infra 1: 学習は再現性があるか?理想としては、同じデータで学習を行った際には同じ結果が期待される。しかし不幸なことに非凸な問題(e.g. Deep Learning)などの学習は再現不可能なことがめずらしくない。シードの固定などをしても必ず再現性が担保されるわけでもない</td><td>モデルのアンサンブルなどは有効である</td></tr><tr><td>Infra 2: モデルの仕様はユニットテストされているか?ユニットテストは外部依存性を排除し、素早く実行可能でなければならない。だが実際はモデルの学習には非常に時間がかかり、計算機資源も必須である</td><td>まずモデルのテストはAPIとしてのテストとアルゴリズムとして正しいかのテストの２つに分解して考える。学習データをフルに活用したモデルの開発は律速になりがちである。そのためベストプラクティスとしては、ランダムに生成した入力データに対して、一つのステップで勾配降下法が上手く動いているかを確認するなどがある。また、モデルの学習過程でチェックポイントを作成しておくことで、学習ジョブが死んだとしても復旧可能になる。機械学習アルゴリズムが正しいかどうかのテストとして、例えばアルゴリズムの特定の部分の計算が正しく実行されているかを確認することができます。別の解決策としては、数回の反復のみを計算し損失関数が減少しているかどうかを確認する。また、モデルのテストを行う際には黄金のテストを避けるべきである。例えば、部分的に学習したモデルと前回のモデルの結果を比較するのはメンテナンスすることが非常に難しい。そして上記のテストはテストが失敗した時に洞察をすることが難しい。</td></tr><tr><td>Infra 3: 全ての機械学習パイプラインは結合テストされているか?完璧な機械学習パイプラインとは主に、学習データの構築 → 特徴生成 → モデル学習 → モデル検証 → サービングシステムへのデプロイから構成される。単一のソフトウェアエンジニアチームは、特定の箇所にのみ集中して開発を行う。そして、その開発によりどこかが壊れパイプライン全体が動かなくなることも珍しくはない。そのため、完全に自動化された結合テストを定常的に行うべきである。</td><td>結合テストをモデルまたはサービングのリリース前に CI として実行する。結合テスト全体がうまく言っているかを目的として高速な結合テストのために、部分的な学習データやシンプルなデータを扱うことを推奨する。また、別途プロダクションに限りなく近づけたミラーリング環境での結合テストも行う</td></tr><tr><td>Infra 4: モデルの品質は、サービングする前に検証されているか?この検証段階で、モデルの受け入れ判定を行う</td><td>同じ検証データを用いて、モデル間の比較を行う。</td></tr><tr><td>Infra 5: 一つのサンプルに対する学習、サービングの段階ごとの計算をデバッグ可能か?モデルの学習を段階ごとに観測する。特定のツールを使うことで観測可能である。例: TensorFlow Debugger</td><td></td></tr><tr><td>Infra 6: カナリーリリースによって、モデルはプロダクション環境下で検証されているか?オフラインテストをいくら広範囲に行ったとしても、プロダクション環境下でのパフォーマンスを保証してくれない。現実世界では、予測不可能な問題が起きる。そのため、常にプロダクション環境下への新モデルのリリースはリスクを生じる。一つの再帰的な問題として、カナリーはモデルの生成物とインフラのミスマッチを把握するのに有用である。モデリングのコードは、サービングのコードと比較して高頻度に変更される。新しいモデリングのコードで、サービングのシステムが動かないことは避けておきたい。図 2 で示すように、Day1 のモデルから Day2 のモデルに更新された際に、そのオペレーターに対応していないサービングシステムが混在することもあり得る。</td><td>ミスマッチの問題を解決するために、一つの取組みとしてプロダクション環境下のサービングのバイナリとインフラに対して新モデルの読み込みを行い、予測ができるかの検証を行う。また一般的にモデルのマイグレーションはカナリリリースにより、新モデルの挙動が問題ないことを確認し、徐々に新モデルの比率を上げて、旧モデルを減らす。そして最終的に旧モデルをゼロにすることでマイグレーションを完了させる</td></tr></tbody></table><p>Infra 7: モデルは、安全かつ高速に前のバージョンに戻せるか?モデルのロールバックが可能かどうかは主にインシデント発生時の対応方法として有用である。モデルのローリバックは、平常時にも常日頃から備えておくべきである。</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="機械学習のモニタリングテスト"></a>機械学習のモニタリングテスト<a aria-hidden="true" tabindex="-1" class="hash-link" href="#機械学習のモニタリングテスト" title="Direct link to heading">#</a></h2><table><thead><tr><th>Monitor</th><th>How</th></tr></thead><tbody><tr><td>Monitor 1. 依存先が変化した結果は通知されるか?機械学習システムは、入力データに強く依存する。学習データや推論時に入力データの傾向や使用が変化した際にも把握しておかないといけない</td><td>開発者チームは、モデルが使用するデータを生み出す依存先をリストアップし依存先に対するアナウンスなどを常に把握しておく。また、依存先のオーナーは、そのデータが機械学習システムに使われていることを知っておく必要もある。</td></tr><tr><td>Monitor 2. 学習とサービングの入力時、両者は普遍性を保っているか？学習データとサービングデータの一貫性を監視することは非常に重要です。機械学習のモデルの振る舞いの異変を把握することは難しいですが、データの普遍性が保たれていないことはまず第一にできる取り組みです。</td><td>スキーマ管理されたデータを取り扱うようにし、その監視も行う。例えば、TensorFlow Data Validation では、スキーマのズレを監視することが可能である。また、このアラートの閾値は適切に設定しないと、Google 内の機械学習システム開発者チームはこのアラートを無視をするようになった逸話もある2。</td></tr><tr><td>Monitor 3. 学習時とサービング時の特徴量計算は同一か？同一の入力に対しても学習時と推論時の一貫性を保っていない、training / serving skewと呼ばれる問題がある。</td><td>この問題を解決するためには、実際のサービングシステムへのトラフィックを保管することが肝心である。サービングシステムへの入力データを保管することで、特徴量生成の比較も容易に行うことができる。確認すべき項目として、特徴量は、必ず学習時と推論時は同一でなければならない。別のアプローチとしては、学習データの統計値と推論時に入力されたデータの統計値を比較することも有用なアプローチである。典型的な統計値としては、最大・最小・平均・欠損値の割合などが挙げられる。</td></tr><tr><td>Monitor 4. モデルは極端に古い状態ではないか？Model 4でも述べたが、モデルの古さは予測精度において重要な指標である。なので、プロダクションで運用されているモデルがどれくらい古いかは監視する必要しておこう。以外にも更新頻度の低いモデルはメンテナンスコストが高い。想像してみよう、例えば一人のエンジニアが手作業で年に 1,2 回再学習されるモデルがあったとする。もしそのエンジニアがチームを離れることになれば、このモデルの再学習の再現は困難になるだろう。たとえ、ドキュメントなどを残していても時間経過により、それも意味を成さなくなる。</td><td>週次、もしくはもっと頻繁にモデルの再学習パイプラインを実行する。プロダクション環境下のモデルの古さは常に監視し、どのレベルの古さが予測精度に影響を与えるかを基準にアラート条件を策定する。</td></tr><tr><td>Monitor 5. モデルは数値的に安定しているか?</td><td>モデルから NaN や無限の値が出力されていないか監視する。</td></tr><tr><td>Monitor 6. このモデルは学習速度や、サービングレイテンシー、スループット、RAM 使用率に影響を与えるようなメモリリークやデグレーションなどは発生していないか?DNN の学習は、計算コストが非常に高く計算機コストの増大はスケーリングのボトルネックにもなる。</td><td>計算使用の監視は、一般的なモニタリングでも重要な指標です。コードのコンポーネントやバージョンごとにモニタリングをするだけではなく、データやモデルも軸にした監視を行う。</td></tr><tr><td>Monitor 7. サービングシステムへの予測品質は測定可能か？</td><td>データの変更や設定の変更などによる予測品質が低下していないことを確認するためのいくつかの方法を説明する。例えば、特定のタスクなどでは、データのラベルが即座に手に入る（広告クリックなど）。このラベルデータを使って、実際のサービングシステムの予測性能をほぼリアルタイムに算出可能である。このようにヒトを介したモデルの評価機構などは Human In The Loop ともよばれる。この仕組によって、定期的に学習のための新たなデータが入手できるようになる。</td></tr></tbody></table><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="おわりに"></a>おわりに<a aria-hidden="true" tabindex="-1" class="hash-link" href="#おわりに" title="Direct link to heading">#</a></h1><p>この記事では、ML Test Scoreを紹介しました。具体的にやってみると、結果は、ML Test Score=0.5で「総合的にテストはされていないが、可能な限り信頼性向上に努めている」と言った温度感のスコアでした。Modelパートが重いので、ぜひ改善して「基礎的なプロジェクトの要求事項は通過した。しかし、信頼性向上のためのさらなる投資が必要とされる」に持っていきたいなと思える評価になりました。ぜひ皆さんお試しください。</p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/mltestscore">mltestscore</a></div><div class="col text--right"><a aria-label="Read more about 機械学習システムの技術的負債を評価する方法論" href="/blog/ml-test-score"><strong>Read More</strong></a></div></footer></article></div></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/data-management">データマネジメントとは</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/bigquery-access-controls">BigQuery アクセス権設定</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Social</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/na0fu3y" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li><li class="footer__item"><a href="https://twitter.com/na0fu3y" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div></div><div class="text--center"><div>Copyright © 2020 Queuery. Built with Docusaurus.</div></div></div></footer></div>
<script src="/styles.4398f2ec.js"></script>
<script src="/runtime~main.8f6c4829.js"></script>
<script src="/main.84f1f6c6.js"></script>
<script src="/1.c2046979.js"></script>
<script src="/2.89f7a116.js"></script>
<script src="/3.0cdb0c07.js"></script>
<script src="/6875c492.2f7c3f5d.js"></script>
<script src="/7f5d2955.8c68583d.js"></script>
<script src="/f610b5ae.676be50c.js"></script>
</body>
</html>